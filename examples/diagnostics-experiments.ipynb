{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import trecs\n",
    "from trecs.models import ContentFiltering, ImplicitMF\n",
    "from trecs.random import Generator\n",
    "from trecs.metrics import MSEMeasurement, RecSimilarity, InteractionSimilarity, RecallMeasurement\n",
    "from trecs.components import Users\n",
    "import trecs.matrix_ops as mo #Note, in order for the ideal recommender to result in non-insane results, the normalize_users parameter in mo.inner_product must be set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://web.ma.utexas.edu/users/mks/statmistakes/skeweddistributions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdealRecommender(ContentFiltering):\n",
    "    \"\"\"\n",
    "    With the Ideal Recommender, we make the *strong assumption* that the true scores are provided\n",
    "    to the recommender system through a custom scoring function, which always returns the true\n",
    "    underlying user-item scores. Therefore, this class is pretty much an empty skeleton; the only\n",
    "    modification is that we don't update any internal state of the recommender at each time step.\n",
    "    \n",
    "    Amy: Note to self, as a part of the baseline ContentFiltering class, if an actual_item_representation \n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        \n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "\n",
    "    def _update_internal_state(self, interactions):\n",
    "        # do not change users_hat!\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1)\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams.update({'font.size': 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the dimensions small for easy visualization\n",
    "number_of_users = 5\n",
    "number_of_attributes = 10\n",
    "number_of_items = 15\n",
    "# We define user_representation using the standard integer generator in Numpy.\n",
    "# We assume a number of interactions with each attribute in the interval [0,4).\n",
    "\n",
    "users_shape = (number_of_users, number_of_attributes)\n",
    "actual_user_representation = Users(size=users_shape, num_users=number_of_users)\n",
    "model_user_representation = np.random.randint(4, size=(number_of_users, number_of_attributes))\n",
    "\n",
    "# We define item_representation using the Generator that comes with the framework\n",
    "# We assume a binary matrix with a binomial distribution\n",
    "\n",
    "actual_item_representation = Generator().binomial(\n",
    "    n=1, p=.3, size=(number_of_attributes, number_of_items)\n",
    ")\n",
    "\n",
    "model_item_representation = Generator().binomial(\n",
    "    n=1, p=.3, size=(number_of_attributes, number_of_items)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 121.76it/s]\n"
     ]
    }
   ],
   "source": [
    "#Not positive this is correct\n",
    "\n",
    "content = ContentFiltering(\n",
    "    num_users=number_of_users,\n",
    "    num_items=number_of_items,\n",
    "    num_attributes=number_of_attributes,\n",
    "    actual_user_representation=actual_user_representation,\n",
    "    user_representation=model_user_representation, \n",
    "    actual_item_representation = actual_item_representation,\n",
    "    item_representation=actual_item_representation, #model has the true item values\n",
    ")\n",
    "# add an MSE measurement\n",
    "content.add_metrics(MSEMeasurement(), RecallMeasurement())\n",
    "# Run for 5 time steps\n",
    "content.run(timesteps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal = IdealRecommender(\n",
    "    num_users=number_of_users,\n",
    "    num_items=number_of_items,\n",
    "    num_attributes=number_of_attributes,\n",
    "    actual_user_representation=actual_user_representation,\n",
    "    user_representation=actual_user_representation.actual_user_profiles.value, \n",
    "    actual_item_representation = actual_item_representation,\n",
    "    item_representation=actual_item_representation, #model has the true item values\n",
    "    num_items_per_iter=\"all\",\n",
    ")\n",
    "# add an MSE measurement\n",
    "ideal.add_metrics(MSEMeasurement(), RecallMeasurement())\n",
    "# Run for 5 time steps\n",
    "ideal.run(timesteps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = ImplicitMF(\n",
    "    num_users=number_of_users,\n",
    "    num_items=number_of_items,\n",
    "    num_latent_factors=number_of_attributes,\n",
    "    actual_user_representation=actual_user_representation,\n",
    "    actual_item_representation = actual_item_representation,\n",
    "    num_items_per_iter=\"all\",\n",
    ")\n",
    "# add an MSE measurement\n",
    "mf.add_metrics(MSEMeasurement(), RecallMeasurement())\n",
    "# Run for 5 time steps\n",
    "#mf.run(timesteps=5,train_between_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 496.54it/s]\n"
     ]
    }
   ],
   "source": [
    "mf.run(timesteps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#top_k_items = np.take(recommender.items_shown, shown_item_ranks[:, k :])\n",
    "\n",
    "#np.log2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22376061 -0.04371299 -0.69470713 -0.9288012   0.32475321 -0.19715885\n",
      " -0.14210062  0.26969499 -0.78559227 -0.59593592 -0.74865705  0.40980589\n",
      " -0.37447204 -0.73084974  0.48053764]\n",
      "\n",
      "\n",
      "[ 3  8 10 13  2  9 12  0  5  6  1  7  4 11 14]\n",
      "\n",
      "\n",
      "[[-0.73084974 -0.69470713 -0.59593592 -0.37447204 -0.22376061 -0.19715885\n",
      "  -0.14210062 -0.04371299  0.26969499  0.32475321  0.40980589  0.48053764]\n",
      " [ 0.32475321 -0.14210062 -0.69470713  0.26969499 -0.04371299 -0.74865705\n",
      "   0.48053764 -0.73084974 -0.19715885 -0.9288012  -0.78559227 -0.22376061]\n",
      " [ 0.26969499  0.32475321  0.48053764  0.40980589 -0.59593592 -0.73084974\n",
      "  -0.22376061 -0.19715885 -0.14210062 -0.37447204 -0.78559227 -0.9288012 ]\n",
      " [ 0.40980589 -0.73084974  0.48053764 -0.69470713 -0.37447204 -0.04371299\n",
      "  -0.74865705 -0.14210062 -0.22376061 -0.78559227 -0.9288012  -0.19715885]\n",
      " [ 0.26969499 -0.37447204  0.32475321 -0.04371299  0.48053764 -0.78559227\n",
      "  -0.9288012  -0.74865705  0.40980589 -0.73084974 -0.69470713 -0.59593592]]\n"
     ]
    }
   ],
   "source": [
    "#recommender.actual_user_item_scores.shape\n",
    "#recommender.predicted_scores.value.shape\n",
    "#print(recommender.items_shown)\n",
    "\n",
    "#print(recommender.items_shown.shape[1])\n",
    "\n",
    "k=3\n",
    "#c = np.tile(np.array(np.arange(0,recommender.num_items), (recommender.num_users,1)))\n",
    "\n",
    "#np.tile(c,(4,1))\n",
    "actual_item_rank = np.argsort(recommender.actual_user_item_scores, axis=1)\n",
    "top_k_items = np.take(recommender.actual_user_item_scores, actual_item_rank[:, recommender.items_shown.shape[1] :])\n",
    "#perfect_item_rel = recommender.actual_user_item_scores\n",
    "#ideal_ranks = np.tile(np.arange(0,recommender.items_shown.shape[1]), (recommender.num_users,1))\n",
    "\n",
    "#idcg = np.sum(recommender.actual_user_item_scores / np.log2(ideal_ranks+2), axis=1)\n",
    "\n",
    "#print(actual_item_rank[:, recommender.items_shown.shape[1] :])\n",
    "print(recommender.actual_user_item_scores[0])\n",
    "print(\"\\n\")\n",
    "print (actual_item_rank[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(np.take(recommender.actual_user_item_scores, actual_item_rank[:, k :]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=5\n",
    "# shown_item_scores = np.take(recommender.predicted_scores.value, recommender.items_shown)\n",
    "# shown_item_ranks = np.argsort(shown_item_scores, axis=1)\n",
    "# top_k_items = np.take(recommender.items_shown, shown_item_ranks[:, k :])\n",
    "\n",
    "# print(shown_item_scores[0])\n",
    "# print(shown_item_ranks[0])\n",
    "# print(shown_item_ranks[0, k :])\n",
    "# print(top_k_items[0])\n",
    "\n",
    "k=7\n",
    "recommender=mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "#This cell calculates recall right now. It is at least correct for a version of MF\n",
    "#Find the scores of the items that were shown \n",
    "shown_item_scores = np.take(recommender.predicted_scores.value, recommender.items_shown)\n",
    "\n",
    "#calcluate their ranks\n",
    "shown_item_ranks = np.argsort(shown_item_scores, axis=1)\n",
    "\n",
    "#argsort sorts in ascending order, so take the last k elements of shown_item_ranks, which will contain the item ids of the top k predicted items\n",
    "top_k_items = np.take(recommender.items_shown, shown_item_ranks[:, recommender.items_shown.shape[1]-k:])\n",
    "\n",
    "#reshape interactions to allow for testing between the interaction array and the top k items array\n",
    "interactions=recommender.interactions.reshape(recommender.num_users,1)\n",
    "recall =len(np.where(interactions==top_k_items)[0]) / recommender.num_users\n",
    "\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14]\n",
      " [ 0]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 9]]\n",
      "[[ 8 10  7  1 12  5 11]\n",
      " [14  9  2  5 11  3  7]\n",
      " [12  5  0  9  8  7 14]\n",
      " [14  7 10 12  5  8  2]\n",
      " [ 6 14  2 12  7  4  1]]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "interactions=recommender.interactions.reshape(recommender.num_users,1)\n",
    "print(interactions)\n",
    "print(top_k_items)\n",
    "print(np.where(interactions==top_k_items)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5]\n",
      " [15]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 7]]\n",
      "(array([0, 1, 2, 3]), array([1, 1, 2, 0]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some useful testing\n",
    "\n",
    "\n",
    "interactions_test = np.array([[5], [15], [8], [5],[7]])\n",
    "print(interactions_test)\n",
    "\n",
    "c=np.where(np.isin(top_k_items, interactions))\n",
    "\n",
    "print(c)\n",
    "\n",
    "np.nonzero(np.in1d(interactions,top_k_items))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  2 14  9 12 11 13  8  3  6]\n",
      " [ 4 10  6 14 12  9 11  8  7  0]\n",
      " [10  2  4 11  6  1  9 12 13 14]\n",
      " [10  6  9  4  8 11  0 14  2 12]\n",
      " [10  9  4 12  6  2 14 11  8  1]]\n",
      "\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "[[0 1 2 3 4 5 6 7 8 9]\n",
      " [0 1 2 3 4 5 6 7 8 9]\n",
      " [0 1 2 3 4 5 6 7 8 9]\n",
      " [0 1 2 3 4 5 6 7 8 9]\n",
      " [0 1 2 3 4 5 6 7 8 9]]\n",
      "[[7 8 9]\n",
      " [7 8 9]\n",
      " [7 8 9]\n",
      " [7 8 9]\n",
      " [7 8 9]]\n",
      "\n",
      "\n",
      "[[8 3 6]\n",
      " [8 3 6]\n",
      " [8 3 6]\n",
      " [8 3 6]\n",
      " [8 3 6]]\n"
     ]
    }
   ],
   "source": [
    "#pd.DataFrame([recommender.items_shown[0].T, ])\n",
    "k=3\n",
    "\n",
    "shown_ids=recommender.items_shown\n",
    "#shown_vals=np.take(recommender.predicted_scores.value, recommender.items_shown)[0]\n",
    "#top_k_items = np.take(recommender.items_shown, shown_item_ranks[0, :k])\n",
    "\n",
    "shown_item_scores = np.take(recommender.predicted_scores.value, recommender.items_shown)\n",
    "shown_item_ranks = np.argsort(shown_item_scores, axis=1)\n",
    "top_k_items = np.take(recommender.items_shown, shown_item_ranks[:, recommender.items_shown.shape[1]-k:])\n",
    "\n",
    "print(shown_ids)\n",
    "print('\\n')\n",
    "print(shown_item_scores)\n",
    "print('\\n')\n",
    "\n",
    "print(shown_item_ranks)\n",
    "\n",
    "print(shown_item_ranks[:, recommender.items_shown.shape[1]-k:])\n",
    "print('\\n')\n",
    "print(top_k_items)\n",
    "\n",
    "#shown_df=pd.DataFrame([shown_ids, shown_vals].T, columns=['shown_ids', 'shown_vals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  8 10 13  2  9 12  0  5  6  1  7  4 11 14]\n",
      " [11  9 12  4  6  2  7  1 10 14 13  5  3  8  0]\n",
      " [ 1  2 10  7  4 14 11  9 13  0  5  6 12  8  3]\n",
      " [ 4  7  9 11 13 14  2 12  1 10  6  0  8  3  5]\n",
      " [ 5  6  0  7 12  4  1 14  8  3 10 11 13  2  9]]\n",
      "[[ 7  9  4  8  0  1  5 10  2 11  3  6 13 14 12]\n",
      " [ 4  8 13 12  7 11 14  0  6 10  3  1  9  5  2]\n",
      " [ 6  1  5  2  9  3  4 11 12 14  0  8 10  7 13]\n",
      " [ 6  4  8  2  1  0  9  7  3 14 10 13 12  5 11]\n",
      " [ 6 11  8  7  4 14 12  0  9 10 13  3  2  1  5]]\n"
     ]
    }
   ],
   "source": [
    "#perfect_items = np.take(recommender.actual_user_item_scores, recommender.items_shown)\n",
    "#print(perfect_items)\n",
    "\n",
    "\n",
    "\n",
    "perfect_item_ranks = np.argsort(recommender.actual_user_item_scores, axis=1)\n",
    "perfect_item_ranks_shown = np.take(perfect_item_ranks, recommender.items_shown)\n",
    "print(perfect_item_ranks)\n",
    "\n",
    "print(perfect_item_ranks_shown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idcg = shown_item_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.70670957  1.70153075  1.04025762  0.89849223  0.84173072  0.50763789\n",
      "  0.4698678   0.43541279  0.37780654  0.04179955 -0.34073811 -0.8941051\n",
      " -1.02131988 -1.47095915 -1.73785371]\n",
      "[[14 13 12 11 10  9  8  7  6  5  4  3  2  1  0]\n",
      " [ 3  6  2  8 10  5 14  9 13 11  7  1  0 12  4]\n",
      " [ 8  9 14  0  5  7  3 12  2  1 10 11  6  4 13]\n",
      " [12  9 11  0  8 14  3 10 13  4  5  2  1  6  7]\n",
      " [ 6  5 10  0 11  1 12  9 14 13  7  2  4  8  3]]\n",
      "[-1.51408158  2.15294635  0.55503203  0.32013311  1.35970687]\n"
     ]
    }
   ],
   "source": [
    "print(shown_item_rel[0])\n",
    "print(shown_item_ranks)\n",
    "#print(shown_item_ranks+2)\n",
    "\n",
    "k=10\n",
    "recommender=mf\n",
    "\n",
    "shown_item_rel = np.take(recommender.predicted_scores.value, recommender.items_shown)\n",
    "actual_rel = np.take()\n",
    "shown_item_ranks = np.argsort(shown_item_rel, axis=1)\n",
    "\n",
    "\n",
    "dcg = np.sum(shown_item_rel / np.log2(shown_item_ranks+2), axis=1)\n",
    "print(dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.users.actual_user_scores.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mf.users.actual_user_scores.shape)\n",
    "print(mf.predicted_user_item_scores.shape)\n",
    "\n",
    "actual = np.reshape(mf.users.actual_user_scores.value, (number_of_users*number_of_items, 1))\n",
    "predicted = np.reshape(mf.predicted_user_item_scores, (number_of_users*number_of_items, 1))\n",
    "print(test.shape)\n",
    "\n",
    "plt.hist(actual, alpha=0.7)\n",
    "plt.hist(predicted, alpha=0.7)#predicted scores are more spread out, which kind of makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_error = abs(actual-predicted)\n",
    "\n",
    "plt.hist(abs_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect measurements about the simulation\n",
    "results = mf.get_measurements()\n",
    "\n",
    "print(\"Results of the simulation:\")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect measurements about the simulation\n",
    "results = ideal.get_measurements()\n",
    "\n",
    "print(\"Results of the simulation:\")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = content.get_measurements()\n",
    "\n",
    "print(\"Results of the simulation:\")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a distribution that hides a subpopulation\n",
    "number_of_attributes = 10\n",
    "number_of_maj_users = 150\n",
    "number_of_min_users = 50\n",
    "\n",
    "maj_user_representation = np.random.normal(1, 2, size=(number_of_maj_users, number_of_attributes))\n",
    "min_user_representation = np.random.normal(0.5, 1.25, size=(number_of_min_users, number_of_attributes))\n",
    "actual_user_representation = np.vstack((maj_user_representation, min_user_representation))\n",
    "split_indices=number_of_maj_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If plotted without respect to the subgroups, preference means look more or less normally distributed\n",
    "plt.hist(actual_user_representation.mean(axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when plotting out mean preferences when accounting for group, we can see a clear distinction in preference\n",
    "plt.hist(maj_user_representation.mean(axis=1), alpha=.7, color='b')\n",
    "plt.hist(min_user_representation.mean(axis=1), alpha=0.7, color='r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering = ContentFiltering(actual_user_representation=actual_user_representation, \n",
    "                             num_attributes=number_of_attributes,\n",
    "                             num_items=500)\n",
    "\n",
    "\n",
    "mse = MSEMeasurement(diagnostics=True)\n",
    "recall=RecallMeasurement()\n",
    "\n",
    "filtering.add_metrics(mse, recall)\n",
    "\n",
    "filtering.startup_and_train(50)\n",
    "filtering.run(450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_diagnostics = filtering.metrics[0].get_diagnostics()\n",
    "mse_beginning = mse_diagnostics.loc[50:, :]\n",
    "mse_beginning.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_histogram(model, split_indices=None):\n",
    "    metric = (\n",
    "                model.predicted_scores.value.mean(axis=1)- model.users.actual_user_scores.value.mean(axis=1))** 2\n",
    "    \n",
    "    colors = [\"blue\", \"orange\", \"red\", \"yellow\", \"green\"]\n",
    "    if split_indices is not None and len(split_indices) > 0:\n",
    "        splits = [0] + split_indices + [metric.size]\n",
    "        for i in range(len(splits) - 1):\n",
    "            values = metric[splits[i] : splits[i + 1]]\n",
    "            plt.hist(values, alpha=0.7, color=colors[i])\n",
    "    else:\n",
    "        plt.hist(metric, bins=\"auto\")\n",
    "        plt.ylabel(\"observation count (total n={})\".format(metric.size))\n",
    "        plt.xlabel(\"mean sqaured error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.startup_and_train(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_histogram(mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.run(50)\n",
    "mf.train()\n",
    "mse_histogram(mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.run(50)\n",
    "mf.train()\n",
    "mse_histogram(mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.run(50)\n",
    "mf.train()\n",
    "mse_histogram(mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.run(50)\n",
    "mse_histogram(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.run(50)\n",
    "mse_histogram(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_k)\n",
    "print(not_in_k)\n",
    "print(len(model.interactions))\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.concatenate((np.ones(len(in_k)), np.zeros(len(not_in_k))), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=mf\n",
    "k=5\n",
    "\n",
    "#split_indices = number_of_maj_users\n",
    "\n",
    "colors = [\"blue\", \"orange\", \"red\", \"yellow\", \"green\"]\n",
    "\n",
    "shown_item_scores = np.take(model.predicted_scores.value, model.items_shown)\n",
    "shown_item_ranks = np.argsort(shown_item_scores, axis=1)\n",
    "top_k_items = np.take(model.items_shown, shown_item_ranks[:, k :])\n",
    "in_k = (np.where(np.isin(model.interactions, top_k_items))[0])\n",
    "not_in_k = (np.where(~np.isin(model.interactions, top_k_items))[0])\n",
    "metric = np.concatenate((np.ones(len(in_k)), np.zeros(len(not_in_k))), axis=None)\n",
    "\n",
    "plt.hist(metric)\n",
    "\n",
    "##Amy, implement this pie chart for recall at k\n",
    "# # Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "# labels = 'Frogs', 'Hogs', 'Dogs', 'Logs'\n",
    "# sizes = [15, 30, 45, 10]\n",
    "# explode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "\n",
    "# fig1, ax1 = plt.subplots()\n",
    "# ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "#         shadow=True, startangle=90)\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# maj_population_outcomes = metric[:split_indices]\n",
    "# min_population_outcomes = metric[split_indices:]\n",
    "\n",
    "# plt.hist(maj_population_outcomes, color=colors[0])\n",
    "# plt.hist(min_population_outcomes, color=colors[1])\n",
    "\n",
    "\n",
    "\n",
    "# if split_indices is not None:\n",
    "#     splits = [0] + split_indices + [metric.size]\n",
    "#     for i in range(len(splits) - 1):\n",
    "#         values = metric[splits[i] : splits[i + 1]]\n",
    "#         plt.hist(values, alpha=0.7, color=colors[i])\n",
    "\n",
    "# plt.hist(metric, bins=\"auto\")\n",
    "# plt.ylabel(\"observation count (total n={})\".format(metric.size))\n",
    "# plt.xlabel(\"recall at k\")\n",
    "\n",
    "\n",
    "# \n",
    "#     if split_indices is not None and len(split_indices) > 0:\n",
    "#         splits = [0] + split_indices + [metric.size]\n",
    "#         for i in range(len(splits) - 1):\n",
    "#             values = metric[splits[i] : splits[i + 1]]\n",
    "#             plt.hist(values, alpha=0.7, color=colors[i])\n",
    "#     else:\n",
    "#         plt.hist(metric, bins=\"auto\")\n",
    "#         plt.ylabel(\"observation count (total n={})\".format(metric.size))\n",
    "#         plt.xlabel(\"mean sqaured error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(min_population_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(min_population_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_histogram(filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_histogram(filtering, [number_of_maj_users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a bimodal distribution\n",
    "N=500\n",
    "mu, sigma = 1.845, 1\n",
    "mu2, sigma2 = 5.845, 1\n",
    "X1 = np.random.normal(mu, sigma, N)\n",
    "X2 = np.random.normal(mu2, sigma2, N)\n",
    "X_bimodal = np.concatenate([X1, X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print majority / minority outcome stats\n",
    "def majority_minority_outcomes(metric, split_index):\n",
    "    split_indices = [split_index]\n",
    "\n",
    "        \n",
    "    maj_mean = metric.last_observation[:split_index].mean()\n",
    "    maj_std = metric.last_observation[:split_index].std()\n",
    "\n",
    "    min_mean = metric.last_observation[split_index:].mean()\n",
    "    min_std = metric.last_observation[split_index:].std()\n",
    "\n",
    "    print(\"Majority group statistics: \", maj_mean, \"(mean), \", maj_std, \"(std)\")\n",
    "    print(\"Minority group statistics: \", min_mean, \"(mean), \", min_std, \"(std)\")\n",
    "    print()\n",
    "    \n",
    "    metric.hist(split_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we expand on this minimal example to gain a deeper understanding of what happens under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering = ContentFiltering(actual_user_representation=actual_user_representation, \n",
    "                             num_attributes=number_of_attributes,\n",
    "                             num_items=500)\n",
    "\n",
    "\n",
    "mse = MSEMeasurement(diagnostics=True)\n",
    "filtering.add_metrics(mse)\n",
    "\n",
    "filtering.startup_and_train(50)\n",
    "majority_minority_outcomes(mse, number_of_maj_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering.run(450)\n",
    "majority_minority_outcomes(mse, number_of_maj_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bimodal = plt.hist(X_bimodal, bins=30)\n",
    "plt.xlabel('Dependent Variable Value')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.title('Bimodal Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(X_bimodal))\n",
    "print(np.std(X_bimodal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "mu, sigma = 14.99, 4\n",
    "X1 = np.random.normal(mu, sigma, N)\n",
    "X_skew = np.log2(X1)\n",
    "\n",
    "skew = plt.hist(X_skew, bins=30)\n",
    "plt.xlabel('Dependent Variable Value')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.title('Skewed Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "mu, sigma = 3.85, 2.2\n",
    "normal_dist = np.random.normal(mu, sigma, N)\n",
    "\n",
    "skew = plt.hist(normal_dist, bins=30)\n",
    "plt.xlabel('Dependent Variable Value')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.title('Normal Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to characterize power, type 1 vs type 2 errors "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trecsEnv",
   "language": "python",
   "name": "trecsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

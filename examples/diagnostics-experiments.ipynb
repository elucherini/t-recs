{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import trecs\n",
    "from trecs.models import ContentFiltering\n",
    "from trecs.random import Generator\n",
    "from trecs.metrics import MSEMeasurement, RecSimilarity, InteractionSimilarity, RecallMeasurement\n",
    "from trecs.components import Users\n",
    "import trecs.matrix_ops as mo #Note, in order for the ideal recommender to result in non-insane results, the normalize_users parameter in mo.inner_product must be set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdealRecommender(ContentFiltering):\n",
    "    \"\"\"\n",
    "    With the Ideal Recommender, we make the *strong assumption* that the true scores are provided\n",
    "    to the recommender system through a custom scoring function, which always returns the true\n",
    "    underlying user-item scores. Therefore, this class is pretty much an empty skeleton; the only\n",
    "    modification is that we don't update any internal state of the recommender at each time step.\n",
    "    \n",
    "    Amy: Note to self, as a part of the baseline ContentFiltering class, if an actual_item_representation \n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        \n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "\n",
    "    def _update_internal_state(self, interactions):\n",
    "        # do not change users_hat!\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1)\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams.update({'font.size': 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the dimensions small for easy visualization\n",
    "number_of_users = 5\n",
    "number_of_attributes = 10\n",
    "number_of_items = 15\n",
    "# We define user_representation using the standard integer generator in Numpy.\n",
    "# We assume a number of interactions with each attribute in the interval [0,4).\n",
    "\n",
    "users_shape = (number_of_users, number_of_attributes)\n",
    "actual_user_representation = Users(size=users_shape, num_users=number_of_users)\n",
    "model_user_representation = np.random.randint(4, size=(number_of_users, number_of_attributes))\n",
    "\n",
    "# We define item_representation using the Generator that comes with the framework\n",
    "# We assume a binary matrix with a binomial distribution\n",
    "\n",
    "actual_item_representation = Generator().binomial(\n",
    "    n=1, p=.3, size=(number_of_attributes, number_of_items)\n",
    ")\n",
    "\n",
    "model_item_representation = Generator().binomial(\n",
    "    n=1, p=.3, size=(number_of_attributes, number_of_items)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Not positive this is correct\n",
    "\n",
    "content = ContentFiltering(\n",
    "    num_users=number_of_users,\n",
    "    num_items=number_of_items,\n",
    "    num_attributes=number_of_attributes,\n",
    "    actual_user_representation=actual_user_representation,\n",
    "    user_representation=model_user_representation, \n",
    "    actual_item_representation = actual_item_representation,\n",
    "    item_representation=actual_item_representation, #model has the true item values\n",
    ")\n",
    "# add an MSE measurement\n",
    "content.add_metrics(MSEMeasurement(), RecallMeasurement())\n",
    "# Run for 5 time steps\n",
    "content.run(timesteps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 1792.74it/s]\n"
     ]
    }
   ],
   "source": [
    "ideal = IdealRecommender(\n",
    "    num_users=number_of_users,\n",
    "    num_items=number_of_items,\n",
    "    num_attributes=number_of_attributes,\n",
    "    actual_user_representation=actual_user_representation,\n",
    "    user_representation=actual_user_representation.actual_user_profiles.value, \n",
    "    actual_item_representation = actual_item_representation,\n",
    "    item_representation=actual_item_representation, #model has the true item values\n",
    "    num_items_per_iter=10,\n",
    ")\n",
    "# add an MSE measurement\n",
    "ideal.add_metrics(MSEMeasurement(), RecallMeasurement())\n",
    "# Run for 5 time steps\n",
    "ideal.run(timesteps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal.actual_item_attributes == ideal.predicted_item_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal.actual_user_profiles == ideal.users_hat.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ideal.items_hat.value.shape)\n",
    "print(ideal.users_hat.value.shape)\n",
    "\n",
    "test=np.dot(ideal.users_hat.value,ideal.items_hat.value)\n",
    "print(test.shape)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ideal.users.actual_user_scores.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 15)\n",
      "(5, 15)\n",
      "(5, 15)\n",
      "[[ 0.          1.26516938  1.65045296 -0.27550128  1.92011014 -1.42721844\n",
      "   0.          0.89672002 -3.16803989 -0.52643267 -1.27202857 -0.49438955\n",
      "  -0.79443041 -0.8726212  -0.32773772]\n",
      " [ 0.         -0.84487896 -3.06453793 -0.13639979 -4.92263048 -0.02228964\n",
      "   0.         -0.14431139 -0.66294351 -0.71648374 -1.5545492  -3.879899\n",
      "  -2.19947617 -3.8867134  -1.98648869]\n",
      " [ 0.          0.44593296  0.16793296  2.14316519 -0.11802906  1.88641389\n",
      "   0.          0.98297234 -0.10660209 -0.23419789  1.05498935  0.06457004\n",
      "  -0.70215349 -2.26259284 -1.08973977]\n",
      " [ 0.          0.64241915  1.12430541 -1.56262308  2.36549033 -3.1283265\n",
      "   0.         -0.25054696 -2.72810574  0.56084514 -2.62257545 -2.82095302\n",
      "  -0.11202093  2.51050699  2.82837921]\n",
      " [ 0.          1.80472856 -3.15938995 -0.43411372 -3.85697692  2.60233285\n",
      "   0.         -1.35406446  2.07142817 -0.74627524 -1.58170818  1.09215876\n",
      "   1.23269782  1.15022388 -0.72867956]]\n",
      "\n",
      "\n",
      "[[ 0.          1.26516938  1.65045296 -0.27550128  1.92011014 -1.42721844\n",
      "   0.          0.89672002 -3.16803989 -0.52643267 -1.27202857 -0.49438955\n",
      "  -0.79443041 -0.8726212  -0.32773772]\n",
      " [ 0.         -0.84487896 -3.06453793 -0.13639979 -4.92263048 -0.02228964\n",
      "   0.         -0.14431139 -0.66294351 -0.71648374 -1.5545492  -3.879899\n",
      "  -2.19947617 -3.8867134  -1.98648869]\n",
      " [ 0.          0.44593296  0.16793296  2.14316519 -0.11802906  1.88641389\n",
      "   0.          0.98297234 -0.10660209 -0.23419789  1.05498935  0.06457004\n",
      "  -0.70215349 -2.26259284 -1.08973977]\n",
      " [ 0.          0.64241915  1.12430541 -1.56262308  2.36549033 -3.1283265\n",
      "   0.         -0.25054696 -2.72810574  0.56084514 -2.62257545 -2.82095302\n",
      "  -0.11202093  2.51050699  2.82837921]\n",
      " [ 0.          1.80472856 -3.15938995 -0.43411372 -3.85697692  2.60233285\n",
      "   0.         -1.35406446  2.07142817 -0.74627524 -1.58170818  1.09215876\n",
      "   1.23269782  1.15022388 -0.72867956]]\n",
      "\n",
      "\n",
      "[[ 0.          1.26516938  1.65045296 -0.27550128  1.92011014 -1.42721844\n",
      "   0.          0.89672002 -3.16803989 -0.52643267 -1.27202857 -0.49438955\n",
      "  -0.79443041 -0.8726212  -0.32773772]\n",
      " [ 0.         -0.84487896 -3.06453793 -0.13639979 -4.92263048 -0.02228964\n",
      "   0.         -0.14431139 -0.66294351 -0.71648374 -1.5545492  -3.879899\n",
      "  -2.19947617 -3.8867134  -1.98648869]\n",
      " [ 0.          0.44593296  0.16793296  2.14316519 -0.11802906  1.88641389\n",
      "   0.          0.98297234 -0.10660209 -0.23419789  1.05498935  0.06457004\n",
      "  -0.70215349 -2.26259284 -1.08973977]\n",
      " [ 0.          0.64241915  1.12430541 -1.56262308  2.36549033 -3.1283265\n",
      "   0.         -0.25054696 -2.72810574  0.56084514 -2.62257545 -2.82095302\n",
      "  -0.11202093  2.51050699  2.82837921]\n",
      " [ 0.          1.80472856 -3.15938995 -0.43411372 -3.85697692  2.60233285\n",
      "   0.         -1.35406446  2.07142817 -0.74627524 -1.58170818  1.09215876\n",
      "   1.23269782  1.15022388 -0.72867956]]\n"
     ]
    }
   ],
   "source": [
    "dot = np.dot(ideal.users_hat.value,ideal.items_hat.value)\n",
    "print(ideal.predicted_scores.shape)\n",
    "print(ideal.users.actual_user_scores.shape)\n",
    "print(dot.shape)\n",
    "\n",
    "#Why aren't these three entities the same?\n",
    "\n",
    "print(ideal.predicted_scores.value)\n",
    "print('\\n')\n",
    "print(ideal.users.actual_user_scores.value)\n",
    "print('\\n')\n",
    "\n",
    "print(np.dot(ideal.users_hat.value,ideal.items_hat.value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are all the same in ideal as of 4/21 at 7PM\n",
    "print(ideal.predicted_user_profiles)\n",
    "print('\\n')\n",
    "print(ideal.users_hat.value)\n",
    "print('\\n')\n",
    "print(ideal.actual_user_profiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are all the same in ideal as of 4/21 at 7PM\n",
    "print(ideal.predicted_item_attributes)\n",
    "print('\\n')\n",
    "print(ideal.items_hat.value)\n",
    "print('\\n')\n",
    "\n",
    "print(ideal.actual_item_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in recommender: \n",
    "# predicted_scores = self.score_fn(\n",
    "#             self.predicted_user_profiles, self.predicted_item_attributes\n",
    "#         )\n",
    "\n",
    "\n",
    "###\n",
    "#     def __init__(  # pylint: disable=R0913,R0912,R0915\n",
    "#         self,\n",
    "#         users_hat,\n",
    "#         items_hat,\n",
    "#         users,\n",
    "#         items,\n",
    "#         num_users,\n",
    "#         num_items,\n",
    "#         num_items_per_iter,\n",
    "#         creators=None,\n",
    "#         probabilistic_recommendations=False,\n",
    "#         measurements=None,\n",
    "#         record_base_state=False,\n",
    "#         system_state=None,\n",
    "#         score_fn=mo.inner_product,\n",
    "#         interleaving_fn=None,\n",
    "#         verbose=False,\n",
    "#         seed=None,\n",
    "#     ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of the simulation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>recall_at_k</th>\n",
       "      <th>timesteps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mse  recall_at_k  timesteps\n",
       "0  0.0          NaN          0\n",
       "1  0.0          0.8          1\n",
       "2  0.0          0.8          2\n",
       "3  0.0          0.8          3\n",
       "4  0.0          0.8          4\n",
       "5  0.0          0.8          5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect measurements about the simulation\n",
    "results = ideal.get_measurements()\n",
    "\n",
    "print(\"Results of the simulation:\")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = content.get_measurements()\n",
    "\n",
    "print(\"Results of the simulation:\")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a distribution that hides a subpopulation\n",
    "number_of_attributes = 10\n",
    "number_of_maj_users = 150\n",
    "number_of_min_users = 50\n",
    "\n",
    "maj_user_representation = np.random.normal(1, 2, size=(number_of_maj_users, number_of_attributes))\n",
    "min_user_representation = np.random.normal(0.5, 1.25, size=(number_of_min_users, number_of_attributes))\n",
    "actual_user_representation = np.vstack((maj_user_representation, min_user_representation))\n",
    "split_indices=number_of_maj_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If plotted without respect to the subgroups, preference means look more or less normally distributed\n",
    "plt.hist(actual_user_representation.mean(axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when plotting out mean preferences when accounting for group, we can see a clear distinction in preference\n",
    "plt.hist(maj_user_representation.mean(axis=1), alpha=.7, color='b')\n",
    "plt.hist(min_user_representation.mean(axis=1), alpha=0.7, color='r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering = ContentFiltering(actual_user_representation=actual_user_representation, \n",
    "                             num_attributes=number_of_attributes,\n",
    "                             num_items=500)\n",
    "\n",
    "\n",
    "mse = MSEMeasurement(diagnostics=True)\n",
    "recall=RecallMeasurement()\n",
    "\n",
    "filtering.add_metrics(mse, recall)\n",
    "\n",
    "filtering.startup_and_train(50)\n",
    "filtering.run(450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_diagnostics = filtering.metrics[0].get_diagnostics()\n",
    "mse_beginning = mse_diagnostics.loc[50:, :]\n",
    "mse_beginning.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_histogram(model, split_indices=None):\n",
    "    metric = (\n",
    "                model.predicted_scores.value.mean(axis=1)- model.users.actual_user_scores.value.mean(axis=1))** 2\n",
    "    \n",
    "    colors = [\"blue\", \"orange\", \"red\", \"yellow\", \"green\"]\n",
    "    if split_indices is not None and len(split_indices) > 0:\n",
    "        splits = [0] + split_indices + [metric.size]\n",
    "        for i in range(len(splits) - 1):\n",
    "            values = metric[splits[i] : splits[i + 1]]\n",
    "            plt.hist(values, alpha=0.7, color=colors[i])\n",
    "    else:\n",
    "        plt.hist(metric, bins=\"auto\")\n",
    "        plt.ylabel(\"observation count (total n={})\".format(metric.size))\n",
    "        plt.xlabel(\"mean sqaured error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_k)\n",
    "print(not_in_k)\n",
    "print(len(model.interactions))\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.concatenate((np.ones(len(in_k)), np.zeros(len(not_in_k))), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"blue\", \"orange\", \"red\", \"yellow\", \"green\"]\n",
    "split_indices = number_of_maj_users\n",
    "shown_item_scores = np.take(model.predicted_scores.value, model.items_shown)\n",
    "shown_item_ranks = np.argsort(shown_item_scores, axis=1)\n",
    "top_k_items = np.take(model.items_shown, shown_item_ranks[:, k :])\n",
    "in_k = (np.where(np.isin(model.interactions, top_k_items))[0])\n",
    "not_in_k = (np.where(~np.isin(model.interactions, top_k_items))[0])\n",
    "metric = np.concatenate((np.ones(len(in_k)), np.zeros(len(not_in_k))), axis=None)\n",
    "\n",
    "maj_population_outcomes = metric[:split_indices]\n",
    "min_population_outcomes = metric[split_indices:]\n",
    "\n",
    "plt.hist(maj_population_outcomes, color=colors[0])\n",
    "plt.hist(min_population_outcomes, color=colors[1])\n",
    "\n",
    "\n",
    "\n",
    "# if split_indices is not None:\n",
    "#     splits = [0] + split_indices + [metric.size]\n",
    "#     for i in range(len(splits) - 1):\n",
    "#         values = metric[splits[i] : splits[i + 1]]\n",
    "#         plt.hist(values, alpha=0.7, color=colors[i])\n",
    "\n",
    "# plt.hist(metric, bins=\"auto\")\n",
    "# plt.ylabel(\"observation count (total n={})\".format(metric.size))\n",
    "# plt.xlabel(\"recall at k\")\n",
    "\n",
    "\n",
    "# \n",
    "#     if split_indices is not None and len(split_indices) > 0:\n",
    "#         splits = [0] + split_indices + [metric.size]\n",
    "#         for i in range(len(splits) - 1):\n",
    "#             values = metric[splits[i] : splits[i + 1]]\n",
    "#             plt.hist(values, alpha=0.7, color=colors[i])\n",
    "#     else:\n",
    "#         plt.hist(metric, bins=\"auto\")\n",
    "#         plt.ylabel(\"observation count (total n={})\".format(metric.size))\n",
    "#         plt.xlabel(\"mean sqaured error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(min_population_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(min_population_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_histogram(filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_histogram(filtering, [number_of_maj_users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a bimodal distribution\n",
    "N=500\n",
    "mu, sigma = 1.845, 1\n",
    "mu2, sigma2 = 5.845, 1\n",
    "X1 = np.random.normal(mu, sigma, N)\n",
    "X2 = np.random.normal(mu2, sigma2, N)\n",
    "X_bimodal = np.concatenate([X1, X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print majority / minority outcome stats\n",
    "def majority_minority_outcomes(metric, split_index):\n",
    "    split_indices = [split_index]\n",
    "\n",
    "        \n",
    "    maj_mean = metric.last_observation[:split_index].mean()\n",
    "    maj_std = metric.last_observation[:split_index].std()\n",
    "\n",
    "    min_mean = metric.last_observation[split_index:].mean()\n",
    "    min_std = metric.last_observation[split_index:].std()\n",
    "\n",
    "    print(\"Majority group statistics: \", maj_mean, \"(mean), \", maj_std, \"(std)\")\n",
    "    print(\"Minority group statistics: \", min_mean, \"(mean), \", min_std, \"(std)\")\n",
    "    print()\n",
    "    \n",
    "    metric.hist(split_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we expand on this minimal example to gain a deeper understanding of what happens under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering = ContentFiltering(actual_user_representation=actual_user_representation, \n",
    "                             num_attributes=number_of_attributes,\n",
    "                             num_items=500)\n",
    "\n",
    "\n",
    "mse = MSEMeasurement(diagnostics=True)\n",
    "filtering.add_metrics(mse)\n",
    "\n",
    "filtering.startup_and_train(50)\n",
    "majority_minority_outcomes(mse, number_of_maj_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering.run(450)\n",
    "majority_minority_outcomes(mse, number_of_maj_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bimodal = plt.hist(X_bimodal, bins=30)\n",
    "plt.xlabel('Dependent Variable Value')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.title('Bimodal Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(X_bimodal))\n",
    "print(np.std(X_bimodal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "mu, sigma = 14.99, 4\n",
    "X1 = np.random.normal(mu, sigma, N)\n",
    "X_skew = np.log2(X1)\n",
    "\n",
    "skew = plt.hist(X_skew, bins=30)\n",
    "plt.xlabel('Dependent Variable Value')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.title('Skewed Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "mu, sigma = 3.85, 2.2\n",
    "normal_dist = np.random.normal(mu, sigma, N)\n",
    "\n",
    "skew = plt.hist(normal_dist, bins=30)\n",
    "plt.xlabel('Dependent Variable Value')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.title('Normal Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to characterize power, type 1 vs type 2 errors "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trecsEnv",
   "language": "python",
   "name": "trecsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

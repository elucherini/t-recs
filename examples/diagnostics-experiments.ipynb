{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import trecs\n",
    "from trecs.models import ContentFiltering, ImplicitMF\n",
    "from trecs.random import Generator\n",
    "from trecs.metrics import MSEMeasurement, RecSimilarity, InteractionSimilarity, RecallMeasurement\n",
    "from trecs.components import Users\n",
    "import trecs.matrix_ops as mo #Note, in order for the ideal recommender to result in non-insane results, the normalize_users parameter in mo.inner_product must be set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://web.ma.utexas.edu/users/mks/statmistakes/skeweddistributions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdealRecommender(ContentFiltering):\n",
    "    \"\"\"\n",
    "    With the Ideal Recommender, we make the *strong assumption* that the true scores are provided\n",
    "    to the recommender system through a custom scoring function, which always returns the true\n",
    "    underlying user-item scores. Therefore, this class is pretty much an empty skeleton; the only\n",
    "    modification is that we don't update any internal state of the recommender at each time step.\n",
    "    \n",
    "    Amy: Note to self, as a part of the baseline ContentFiltering class, if an actual_item_representation \n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        \n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "\n",
    "    def _update_internal_state(self, interactions):\n",
    "        # do not change users_hat!\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1)\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams.update({'font.size': 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the dimensions small for easy visualization\n",
    "number_of_users = 5\n",
    "number_of_attributes = 10\n",
    "number_of_items = 15\n",
    "# We define user_representation using the standard integer generator in Numpy.\n",
    "# We assume a number of interactions with each attribute in the interval [0,4).\n",
    "\n",
    "users_shape = (number_of_users, number_of_attributes)\n",
    "actual_user_representation = Users(size=users_shape, num_users=number_of_users)\n",
    "model_user_representation = np.random.randint(4, size=(number_of_users, number_of_attributes))\n",
    "\n",
    "# We define item_representation using the Generator that comes with the framework\n",
    "# We assume a binary matrix with a binomial distribution\n",
    "\n",
    "actual_item_representation = Generator().binomial(\n",
    "    n=1, p=.3, size=(number_of_attributes, number_of_items)\n",
    ")\n",
    "\n",
    "model_item_representation = Generator().binomial(\n",
    "    n=1, p=.3, size=(number_of_attributes, number_of_items)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Not positive this is correct\n",
    "\n",
    "content = ContentFiltering(\n",
    "    num_users=number_of_users,\n",
    "    num_items=number_of_items,\n",
    "    num_attributes=number_of_attributes,\n",
    "    actual_user_representation=actual_user_representation,\n",
    "    user_representation=model_user_representation, \n",
    "    actual_item_representation = actual_item_representation,\n",
    "    item_representation=actual_item_representation, #model has the true item values\n",
    ")\n",
    "# add an MSE measurement\n",
    "content.add_metrics(MSEMeasurement(), RecallMeasurement())\n",
    "# Run for 5 time steps\n",
    "content.run(timesteps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal = IdealRecommender(\n",
    "    num_users=number_of_users,\n",
    "    num_items=number_of_items,\n",
    "    num_attributes=number_of_attributes,\n",
    "    actual_user_representation=actual_user_representation,\n",
    "    user_representation=actual_user_representation.actual_user_profiles.value, \n",
    "    actual_item_representation = actual_item_representation,\n",
    "    item_representation=actual_item_representation, #model has the true item values\n",
    "    num_items_per_iter=\"all\",\n",
    ")\n",
    "# add an MSE measurement\n",
    "ideal.add_metrics(MSEMeasurement(), RecallMeasurement())\n",
    "# Run for 5 time steps\n",
    "ideal.run(timesteps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = ImplicitMF(\n",
    "    num_users=number_of_users,\n",
    "    num_items=number_of_items,\n",
    "    num_latent_factors=number_of_attributes,\n",
    "    actual_user_representation=actual_user_representation,\n",
    "    actual_item_representation = actual_item_representation,\n",
    "    num_items_per_iter=\"all\",\n",
    ")\n",
    "# add an MSE measurement\n",
    "mf.add_metrics(MSEMeasurement(), RecallMeasurement())\n",
    "# Run for 5 time steps\n",
    "#mf.run(timesteps=5,train_between_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 496.54it/s]\n"
     ]
    }
   ],
   "source": [
    "mf.run(timesteps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "recommender=mf\n",
    "\n",
    "shown_item_rel = np.take(recommender.predicted_scores.value, recommender.items_shown)\n",
    "shown_item_ranks = np.argsort(shown_item_rel, axis=1)\n",
    "top_k_items = np.take(recommender.items_shown, shown_item_ranks[:, k :])\n",
    "\n",
    "#np.log2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ideal_item_ranks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-19a941aa6a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mideal_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems_shown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0midcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactual_user_item_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mideal_item_ranks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midcg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ideal_item_ranks' is not defined"
     ]
    }
   ],
   "source": [
    "#recommender.actual_user_item_scores.shape\n",
    "#recommender.predicted_scores.value.shape\n",
    "#print(recommender.items_shown)\n",
    "\n",
    "#print(recommender.items_shown.shape[1])\n",
    "\n",
    "\n",
    "#c = np.tile(np.array(np.arange(0,recommender.num_items), (recommender.num_users,1)))\n",
    "\n",
    "#np.tile(c,(4,1))\n",
    "perfect_item_rel = recommender.actual_user_item_scores\n",
    "ideal_ranks = np.tile(np.arange(0,recommender.items_shown.shape[1]), (recommender.num_users,1))\n",
    "\n",
    "idcg = recommender.actual_user_item_scores / np.log2(ideal_item_ranks+2)\n",
    "\n",
    "print(idcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  8 10 13  2  9 12  0  5  6  1  7  4 11 14]\n",
      " [11  9 12  4  6  2  7  1 10 14 13  5  3  8  0]\n",
      " [ 1  2 10  7  4 14 11  9 13  0  5  6 12  8  3]\n",
      " [ 4  7  9 11 13 14  2 12  1 10  6  0  8  3  5]\n",
      " [ 5  6  0  7 12  4  1 14  8  3 10 11 13  2  9]]\n",
      "[[ 7  9  4  8  0  1  5 10  2 11  3  6 13 14 12]\n",
      " [ 4  8 13 12  7 11 14  0  6 10  3  1  9  5  2]\n",
      " [ 6  1  5  2  9  3  4 11 12 14  0  8 10  7 13]\n",
      " [ 6  4  8  2  1  0  9  7  3 14 10 13 12  5 11]\n",
      " [ 6 11  8  7  4 14 12  0  9 10 13  3  2  1  5]]\n"
     ]
    }
   ],
   "source": [
    "#perfect_items = np.take(recommender.actual_user_item_scores, recommender.items_shown)\n",
    "#print(perfect_items)\n",
    "\n",
    "\n",
    "\n",
    "perfect_item_ranks = np.argsort(recommender.actual_user_item_scores, axis=1)\n",
    "perfect_item_ranks_shown = np.take(perfect_item_ranks, recommender.items_shown)\n",
    "print(perfect_item_ranks)\n",
    "\n",
    "print(perfect_item_ranks_shown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idcg = shown_item_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.70670957  1.70153075  1.04025762  0.89849223  0.84173072  0.50763789\n",
      "  0.4698678   0.43541279  0.37780654  0.04179955 -0.34073811 -0.8941051\n",
      " -1.02131988 -1.47095915 -1.73785371]\n",
      "[[14 13 12 11 10  9  8  7  6  5  4  3  2  1  0]\n",
      " [ 3  6  2  8 10  5 14  9 13 11  7  1  0 12  4]\n",
      " [ 8  9 14  0  5  7  3 12  2  1 10 11  6  4 13]\n",
      " [12  9 11  0  8 14  3 10 13  4  5  2  1  6  7]\n",
      " [ 6  5 10  0 11  1 12  9 14 13  7  2  4  8  3]]\n",
      "[[16 15 14 13 12 11 10  9  8  7  6  5  4  3  2]\n",
      " [ 5  8  4 10 12  7 16 11 15 13  9  3  2 14  6]\n",
      " [10 11 16  2  7  9  5 14  4  3 12 13  8  6 15]\n",
      " [14 11 13  2 10 16  5 12 15  6  7  4  3  8  9]\n",
      " [ 8  7 12  2 13  3 14 11 16 15  9  4  6 10  5]]\n",
      "[[ 0.42667739  0.43552045  0.27322318  0.24280688  0.23479485  0.14674026\n",
      "   0.1414443   0.13735744  0.12593551  0.0148893  -0.1318155  -0.38507011\n",
      "  -0.51065994 -0.92807189 -1.73785371]\n",
      " [ 0.44801457  0.29949741 -0.51065994 -0.5231461   0.4760746   0.0148893\n",
      "  -0.36773979  0.24331474 -0.22885337  0.11766515 -0.10749091  0.32028385\n",
      "   1.70153075  0.12341056  0.14615552]\n",
      " [-0.26915245  0.14674026  0.11746695  0.37780654  0.60609748 -0.10749091\n",
      "   0.44801457  0.01097863 -0.86892686 -0.92807189  0.23479485  0.24280688\n",
      "   0.1451376   0.66024539 -0.26141502]\n",
      " [-0.23483629  0.30070189  0.24280688  0.37780654  0.15281423  0.21043268\n",
      "   0.73280941  0.4760746  -0.08721465 -0.56904468  0.15509717 -0.51065994\n",
      "  -1.09646362  0.1566226   0.01318629]\n",
      " [-0.29803503  0.0148893   0.25062807  1.70670957  0.2811173  -0.92807189\n",
      "  -0.45644647  0.24331474  0.42538269  0.1114474  -0.32219055 -0.17036906\n",
      "   0.14615552  0.15281423  0.20236105]]\n"
     ]
    }
   ],
   "source": [
    "print(shown_item_rel[0])\n",
    "print(shown_item_ranks)\n",
    "#print(shown_item_ranks+2)\n",
    "\n",
    "\n",
    "dcg = shown_item_rel / np.log2(shown_item_ranks+2)\n",
    "print(dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.users.actual_user_scores.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mf.users.actual_user_scores.shape)\n",
    "print(mf.predicted_user_item_scores.shape)\n",
    "\n",
    "actual = np.reshape(mf.users.actual_user_scores.value, (number_of_users*number_of_items, 1))\n",
    "predicted = np.reshape(mf.predicted_user_item_scores, (number_of_users*number_of_items, 1))\n",
    "print(test.shape)\n",
    "\n",
    "plt.hist(actual, alpha=0.7)\n",
    "plt.hist(predicted, alpha=0.7)#predicted scores are more spread out, which kind of makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_error = abs(actual-predicted)\n",
    "\n",
    "plt.hist(abs_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect measurements about the simulation\n",
    "results = mf.get_measurements()\n",
    "\n",
    "print(\"Results of the simulation:\")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect measurements about the simulation\n",
    "results = ideal.get_measurements()\n",
    "\n",
    "print(\"Results of the simulation:\")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = content.get_measurements()\n",
    "\n",
    "print(\"Results of the simulation:\")\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a distribution that hides a subpopulation\n",
    "number_of_attributes = 10\n",
    "number_of_maj_users = 150\n",
    "number_of_min_users = 50\n",
    "\n",
    "maj_user_representation = np.random.normal(1, 2, size=(number_of_maj_users, number_of_attributes))\n",
    "min_user_representation = np.random.normal(0.5, 1.25, size=(number_of_min_users, number_of_attributes))\n",
    "actual_user_representation = np.vstack((maj_user_representation, min_user_representation))\n",
    "split_indices=number_of_maj_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If plotted without respect to the subgroups, preference means look more or less normally distributed\n",
    "plt.hist(actual_user_representation.mean(axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when plotting out mean preferences when accounting for group, we can see a clear distinction in preference\n",
    "plt.hist(maj_user_representation.mean(axis=1), alpha=.7, color='b')\n",
    "plt.hist(min_user_representation.mean(axis=1), alpha=0.7, color='r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering = ContentFiltering(actual_user_representation=actual_user_representation, \n",
    "                             num_attributes=number_of_attributes,\n",
    "                             num_items=500)\n",
    "\n",
    "\n",
    "mse = MSEMeasurement(diagnostics=True)\n",
    "recall=RecallMeasurement()\n",
    "\n",
    "filtering.add_metrics(mse, recall)\n",
    "\n",
    "filtering.startup_and_train(50)\n",
    "filtering.run(450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_diagnostics = filtering.metrics[0].get_diagnostics()\n",
    "mse_beginning = mse_diagnostics.loc[50:, :]\n",
    "mse_beginning.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_histogram(model, split_indices=None):\n",
    "    metric = (\n",
    "                model.predicted_scores.value.mean(axis=1)- model.users.actual_user_scores.value.mean(axis=1))** 2\n",
    "    \n",
    "    colors = [\"blue\", \"orange\", \"red\", \"yellow\", \"green\"]\n",
    "    if split_indices is not None and len(split_indices) > 0:\n",
    "        splits = [0] + split_indices + [metric.size]\n",
    "        for i in range(len(splits) - 1):\n",
    "            values = metric[splits[i] : splits[i + 1]]\n",
    "            plt.hist(values, alpha=0.7, color=colors[i])\n",
    "    else:\n",
    "        plt.hist(metric, bins=\"auto\")\n",
    "        plt.ylabel(\"observation count (total n={})\".format(metric.size))\n",
    "        plt.xlabel(\"mean sqaured error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.startup_and_train(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_histogram(mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.run(50)\n",
    "mf.train()\n",
    "mse_histogram(mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.run(50)\n",
    "mf.train()\n",
    "mse_histogram(mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.run(50)\n",
    "mf.train()\n",
    "mse_histogram(mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.run(50)\n",
    "mse_histogram(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.run(50)\n",
    "mse_histogram(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_k)\n",
    "print(not_in_k)\n",
    "print(len(model.interactions))\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.concatenate((np.ones(len(in_k)), np.zeros(len(not_in_k))), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=mf\n",
    "k=5\n",
    "\n",
    "#split_indices = number_of_maj_users\n",
    "\n",
    "colors = [\"blue\", \"orange\", \"red\", \"yellow\", \"green\"]\n",
    "\n",
    "shown_item_scores = np.take(model.predicted_scores.value, model.items_shown)\n",
    "shown_item_ranks = np.argsort(shown_item_scores, axis=1)\n",
    "top_k_items = np.take(model.items_shown, shown_item_ranks[:, k :])\n",
    "in_k = (np.where(np.isin(model.interactions, top_k_items))[0])\n",
    "not_in_k = (np.where(~np.isin(model.interactions, top_k_items))[0])\n",
    "metric = np.concatenate((np.ones(len(in_k)), np.zeros(len(not_in_k))), axis=None)\n",
    "\n",
    "plt.hist(metric)\n",
    "\n",
    "##Amy, implement this pie chart for recall at k\n",
    "# # Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "# labels = 'Frogs', 'Hogs', 'Dogs', 'Logs'\n",
    "# sizes = [15, 30, 45, 10]\n",
    "# explode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "\n",
    "# fig1, ax1 = plt.subplots()\n",
    "# ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "#         shadow=True, startangle=90)\n",
    "# ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# maj_population_outcomes = metric[:split_indices]\n",
    "# min_population_outcomes = metric[split_indices:]\n",
    "\n",
    "# plt.hist(maj_population_outcomes, color=colors[0])\n",
    "# plt.hist(min_population_outcomes, color=colors[1])\n",
    "\n",
    "\n",
    "\n",
    "# if split_indices is not None:\n",
    "#     splits = [0] + split_indices + [metric.size]\n",
    "#     for i in range(len(splits) - 1):\n",
    "#         values = metric[splits[i] : splits[i + 1]]\n",
    "#         plt.hist(values, alpha=0.7, color=colors[i])\n",
    "\n",
    "# plt.hist(metric, bins=\"auto\")\n",
    "# plt.ylabel(\"observation count (total n={})\".format(metric.size))\n",
    "# plt.xlabel(\"recall at k\")\n",
    "\n",
    "\n",
    "# \n",
    "#     if split_indices is not None and len(split_indices) > 0:\n",
    "#         splits = [0] + split_indices + [metric.size]\n",
    "#         for i in range(len(splits) - 1):\n",
    "#             values = metric[splits[i] : splits[i + 1]]\n",
    "#             plt.hist(values, alpha=0.7, color=colors[i])\n",
    "#     else:\n",
    "#         plt.hist(metric, bins=\"auto\")\n",
    "#         plt.ylabel(\"observation count (total n={})\".format(metric.size))\n",
    "#         plt.xlabel(\"mean sqaured error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(min_population_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(min_population_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_histogram(filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_histogram(filtering, [number_of_maj_users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a bimodal distribution\n",
    "N=500\n",
    "mu, sigma = 1.845, 1\n",
    "mu2, sigma2 = 5.845, 1\n",
    "X1 = np.random.normal(mu, sigma, N)\n",
    "X2 = np.random.normal(mu2, sigma2, N)\n",
    "X_bimodal = np.concatenate([X1, X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print majority / minority outcome stats\n",
    "def majority_minority_outcomes(metric, split_index):\n",
    "    split_indices = [split_index]\n",
    "\n",
    "        \n",
    "    maj_mean = metric.last_observation[:split_index].mean()\n",
    "    maj_std = metric.last_observation[:split_index].std()\n",
    "\n",
    "    min_mean = metric.last_observation[split_index:].mean()\n",
    "    min_std = metric.last_observation[split_index:].std()\n",
    "\n",
    "    print(\"Majority group statistics: \", maj_mean, \"(mean), \", maj_std, \"(std)\")\n",
    "    print(\"Minority group statistics: \", min_mean, \"(mean), \", min_std, \"(std)\")\n",
    "    print()\n",
    "    \n",
    "    metric.hist(split_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we expand on this minimal example to gain a deeper understanding of what happens under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering = ContentFiltering(actual_user_representation=actual_user_representation, \n",
    "                             num_attributes=number_of_attributes,\n",
    "                             num_items=500)\n",
    "\n",
    "\n",
    "mse = MSEMeasurement(diagnostics=True)\n",
    "filtering.add_metrics(mse)\n",
    "\n",
    "filtering.startup_and_train(50)\n",
    "majority_minority_outcomes(mse, number_of_maj_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering.run(450)\n",
    "majority_minority_outcomes(mse, number_of_maj_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bimodal = plt.hist(X_bimodal, bins=30)\n",
    "plt.xlabel('Dependent Variable Value')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.title('Bimodal Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(X_bimodal))\n",
    "print(np.std(X_bimodal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "mu, sigma = 14.99, 4\n",
    "X1 = np.random.normal(mu, sigma, N)\n",
    "X_skew = np.log2(X1)\n",
    "\n",
    "skew = plt.hist(X_skew, bins=30)\n",
    "plt.xlabel('Dependent Variable Value')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.title('Skewed Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "mu, sigma = 3.85, 2.2\n",
    "normal_dist = np.random.normal(mu, sigma, N)\n",
    "\n",
    "skew = plt.hist(normal_dist, bins=30)\n",
    "plt.xlabel('Dependent Variable Value')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.title('Normal Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to characterize power, type 1 vs type 2 errors "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trecsEnv",
   "language": "python",
   "name": "trecsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
